{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1sig1qLUJwNPLi7L7UjFGY5PkMwNtGLXj",
      "authorship_tag": "ABX9TyPbBi1dwBcDMnqnrd8UNmix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnodking7/PINN-California-Delta/blob/main/PINN_ANN_hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to find a set of optimal hyperparameters for ANN (fully-connected) and a set of optimal hyperparameters for PINN (Physics-informed neural network).\n",
        "\n",
        "Both NN models estimate the salinity at locations Martinez, Port Chicago, Chipps Island stations in California-Delta estuary using the outflow data at these three locations as inputs.\n",
        "\n",
        "The main difference between ANN model and PINN model is illustrated by the following NN structure:\n",
        "\n",
        "\n",
        "\n",
        "1.   ANN\n",
        "\n",
        "![](https://drive.google.com/uc?id=1pPT0dd_v91PRnfj7lw8aLIzWZu0T9Rcu)\n",
        "\n",
        "*   The input of ANN is the outflow vector $\\vec{Q}_n$ corresponding to day $n$.\n",
        "*   ANN is trained by minimizing the mean squared error $$\\sum_n\\|\\hat{S}_n-S_n\\|^2$$ where $\\hat{S}_n$ is the output of ANN and $S_n$ is the true salinity value.\n",
        "\n",
        "2.   PINN\n",
        "\n",
        "![](https://drive.google.com/uc?id=1Z77TmoPzrmUN3SdADFWhaD22KXC9LY5f)\n",
        "\n",
        "*   The input of PINN is the outflow vector $\\vec{Q}_n$ **and** location $x_n$ and time $t_n$.\n",
        "*   PINN is trained by minimizing the mean squared error **and** PDE(1D Advection Dispersion eq) loss $$\\sum_n\\|\\hat{S}_n-S_n\\|^2 +\\sum_n\\bigg\\|A\\frac{\\partial \\hat{S}}{\\partial t}\\Bigr|_{(x_n,t_n)}-\\vec{Q}_{n,1}\\frac{\\partial \\hat{S}}{\\partial x}\\Bigr|_{(x_n,t_n)}-KA\\frac{\\partial^2 \\hat{S}}{\\partial x^2}\\Bigr|_{(x_n,t_n)}\\bigg\\|^2$$ where $\\vec{Q}_{n,1}$ is the first component of the outflow vector $\\vec{Q}_n$."
      ],
      "metadata": {
        "id": "PtnBXbftIuTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary Setup"
      ],
      "metadata": {
        "id": "iRQOEDZiLx9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages\n"
      ],
      "metadata": {
        "id": "IoV30wEztavU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8z7TvEgLhkb",
        "outputId": "b268f747-677c-49cf-9f8c-f8d6a192ab04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 30.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 45.6 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepxde\n",
            "  Downloading DeepXDE-1.7.2-py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 17.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from deepxde) (1.0.2)\n",
            "Collecting scikit-optimize>=0.9.0\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from deepxde) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from deepxde) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepxde) (1.21.6)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize>=0.9.0->deepxde) (1.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyaml>=16.9->scikit-optimize>=0.9.0->deepxde) (6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->deepxde) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->deepxde) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->deepxde) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->deepxde) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->deepxde) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->deepxde) (1.15.0)\n",
            "Installing collected packages: pyaml, scikit-optimize, deepxde\n",
            "Successfully installed deepxde-1.7.2 pyaml-21.10.1 scikit-optimize-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-tuner\n",
        "!pip install deepxde"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import deepxde as dde"
      ],
      "metadata": {
        "id": "VIZIcM914BO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cbd0f2-79b7-4273-c9f3-9568e7a06d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DeepXDE backend not selected or invalid. Use tensorflow.compat.v1.\n",
            "Using backend: tensorflow.compat.v1\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"tensorflow.compat.v1\". You can change it in the ~/.deepxde/config.json file or export the DDE_BACKEND environment variable. Valid options are: tensorflow.compat.v1, tensorflow, pytorch, jax, paddle (all lowercase)\n",
            "Enable just-in-time compilation with XLA.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/deepxde/nn/initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as np\n",
        "import matplotlib.pylab as pl\n",
        "import pandas as pd\n",
        "import sys"
      ],
      "metadata": {
        "id": "f88ORrDgL8MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data"
      ],
      "metadata": {
        "id": "QoY_y4iQthep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read outflow and salinity data"
      ],
      "metadata": {
        "id": "K3DBtgnbx1NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/drive/My Drive/MAC/Python')"
      ],
      "metadata": {
        "id": "ySXVLv85L_my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The data is in an Excel file.\n",
        "Daily DSM2 outflow and salinity at locations:\n",
        "  Martinez  Port Chicago  Chipps Island\n",
        "from Apr 1 1990 to Dec 28 2017\n",
        "'''\n",
        "df = pd.read_excel(\"/content/drive/My Drive/MAC/Python/PortChicago202210.xlsx\", header=None)\n",
        "All = np.asarray(df)\n",
        "All = All[7:, :]  # Discard headers\n",
        "\n",
        "for i in range(len(All)):\n",
        "    All[i, 1] = str(All[i, 1])  # Change dates to str\n",
        "\n",
        "import numpy"
      ],
      "metadata": {
        "id": "zzTfXUVVL_zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_pre(year, daily, window, window_len):\n",
        "    \"\"\"\n",
        "    Returns preprocessed outflow data for a given 'year' at locations\n",
        "    Martinez, Port Chicago, Chipps Island such that\n",
        "    to each day corresponds outflow data of length 'daily' + 'window'\n",
        "\n",
        "    The preprocessed outflow data vector is created such that\n",
        "    first 'daily' number of outflow (including current day) is included\n",
        "    and prior 'window_len' many outflows are averaged 'window' many times\n",
        "\n",
        "    PARAMETERS\n",
        "    ----------\n",
        "    year :        string or array of strings\n",
        "                  Outflow year(s)\n",
        "    daily :       int\n",
        "                  number of outflow to include 'as-is'\n",
        "    window :      int\n",
        "                  number of average windows in return outflow\n",
        "    window_len :  int\n",
        "                  length of each window\n",
        "    RETURNS\n",
        "    -------\n",
        "    M_OUT :       ndarray, shape (year length, daily+window)\n",
        "                  Preprocessed outflow at Martinez\n",
        "    P_OUT :       ndarray, shape (year length, daily+window)\n",
        "                  Preprocessed outflow at Port Chicago\n",
        "    C_OUT :       ndarray, shape (year length, daily+window)\n",
        "                  Preprocessed outflow at Chipps Island\n",
        "    \"\"\"\n",
        "\n",
        "    if type(year) == str:\n",
        "        indices = numpy.flatnonzero(numpy.core.defchararray.find(list(All[:, 1]), year)!=-1)\n",
        "    else:\n",
        "        indices = []\n",
        "        for yr in year:\n",
        "            indices.extend(numpy.flatnonzero(numpy.core.defchararray.find(list(All[:, 1]), yr)!=-1))\n",
        "    total = daily + window * window_len\n",
        "    OUT_d = All[indices[0] - (total - 1):indices[-1] + 1, 2:5] # Range of outflow to extract outflow vector from\n",
        "    OUT_d = np.flip(OUT_d, 1) # Locations are in reverse order, so flip data\n",
        "    OUT_d = (OUT_d - np.min(OUT_d)) / (np.max(OUT_d) - np.min(OUT_d)) # Normalize\n",
        "\n",
        "    ## Preprocessed outflow data for Martinez\n",
        "    M_OUT = []\n",
        "    cur = total - 1\n",
        "    for i in range(len(indices)):\n",
        "        vec = OUT_d[cur-(daily-1):cur+1, 0]\n",
        "        vec = np.flip(vec)\n",
        "        ind = cur - (daily - 1)\n",
        "        for j in range(window):\n",
        "            avg = np.mean(OUT_d[ind-window_len:ind, 0])\n",
        "            vec = np.concatenate((vec, np.array([avg])))\n",
        "            ind -= window_len\n",
        "        M_OUT.append(vec)\n",
        "        cur += 1\n",
        "    M_OUT = np.array(M_OUT)\n",
        "\n",
        "    ## Preprocessed outflow data for Port Chicago\n",
        "    P_OUT = []\n",
        "    cur = total - 1\n",
        "    for i in range(len(indices)):\n",
        "        vec = OUT_d[cur-(daily-1):cur+1, 1]\n",
        "        vec = np.flip(vec)\n",
        "        ind = cur - (daily - 1)\n",
        "        for j in range(window):\n",
        "            avg = np.mean(OUT_d[ind-window_len:ind, 1])\n",
        "            vec = np.concatenate((vec, np.array([avg])))\n",
        "            ind -= window_len\n",
        "        P_OUT.append(vec)\n",
        "        cur += 1\n",
        "    P_OUT = np.array(P_OUT)\n",
        "\n",
        "    ## Preprocessed outflow data for Chipps Island\n",
        "    C_OUT = []\n",
        "    cur = total - 1\n",
        "    for i in range(len(indices)):\n",
        "        vec = OUT_d[cur-(daily-1):cur+1, 2]\n",
        "        vec = np.flip(vec)\n",
        "        ind = cur - (daily - 1)\n",
        "        for j in range(window):\n",
        "            avg = np.mean(OUT_d[ind-window_len:ind, 2])\n",
        "            vec = np.concatenate((vec, np.array([avg])))\n",
        "            ind -= window_len\n",
        "        C_OUT.append(vec)\n",
        "        cur += 1\n",
        "    C_OUT = np.array(C_OUT)\n",
        "\n",
        "    return M_OUT, P_OUT, C_OUT"
      ],
      "metadata": {
        "id": "0vag9R9uMBka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess to create outflow vectors"
      ],
      "metadata": {
        "id": "QItuIrgNyDca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years = ['1991','1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017']\n",
        "\n",
        "indices = []\n",
        "for yr in years:\n",
        "    indices.extend(numpy.flatnonzero(numpy.core.defchararray.find(list(All[:, 1]), yr)!=-1))\n",
        "indices = np.array(indices)\n",
        "\n",
        "EC_d = All[indices, 5:] # Salinity at three locations\n",
        "EC_min = np.min(EC_d)\n",
        "EC_max = np.max(EC_d)\n",
        "\n",
        "'''\n",
        "Create outflow data vectors at each location\n",
        "To each day corresponds a 18-dim vector created from antecedent 118 outflow data:\n",
        "  current day + previous 7 days + previous 110 days averaged into 10-dim values with window length 11\n",
        "'''\n",
        "M_OUT, P_OUT, C_OUT = data_pre(years, 8, 10, 11)"
      ],
      "metadata": {
        "id": "_vYV8yEHCGLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split to train and validation"
      ],
      "metadata": {
        "id": "3WAqnUGqyLDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_years = ['1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017']\n",
        "# tr_years = ['1991','1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011']\n",
        "tst_years = ['1991','1992','1993','1994','1995','1996']\n",
        "# tst_years = ['2012','2013','2014','2015','2016','2017']\n",
        "\n",
        "tr_indices = []\n",
        "for yr in tr_years:\n",
        "    tr_indices.extend(numpy.flatnonzero(numpy.core.defchararray.find(list(All[:, 1]), yr)!=-1))\n",
        "tr_indices = np.array(tr_indices)\n",
        "\n",
        "tst_indices = []\n",
        "for yr in tst_years:\n",
        "    tst_indices.extend(numpy.flatnonzero(numpy.core.defchararray.find(list(All[:, 1]), yr)!=-1))\n",
        "tst_indices = np.array(tst_indices)"
      ],
      "metadata": {
        "id": "BqJCXwn_Chf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training data\n",
        "tr_EC_d = All[tr_indices, 5:]\n",
        "tr_EC_d = np.flip(tr_EC_d, 1)\n",
        "tr_EC_d = (tr_EC_d - EC_min) / (EC_max - EC_min) # Normalize\n",
        "\n",
        "Q = []  # Inputs for ANN\n",
        "X = []  # Inputs for PINN\n",
        "Svals = []  # Outputs (same for both ANN and PINN)\n",
        "\n",
        "portion = 1 / (len(tr_indices) + len(tst_indices) # to split the time domain [0, 1] uniformly\n",
        "\n",
        "for i in range(len(tr_EC_d)):\n",
        "    cur_ind = tr_indices[i]\n",
        "    for j in range(3):\n",
        "        # Martinez\n",
        "        if j == 0:\n",
        "            Q.append(M_OUT[cur_ind-275, :])\n",
        "            X.append(np.concatenate(([0, (cur_ind - 274) * portion], M_OUT[cur_ind-275, :])))\n",
        "        # Port Chicago\n",
        "        elif j == 1:\n",
        "            Q.append(P_OUT[cur_ind-275, :])\n",
        "            X.append(np.concatenate(([0.442, (cur_ind - 274) * portion], P_OUT[cur_ind-275, :])))\n",
        "        # Chipps Island\n",
        "        elif j == 2:\n",
        "            Q.append(C_OUT[cur_ind-275, :])\n",
        "            X.append(np.concatenate(([1, (cur_ind - 274) * portion], C_OUT[cur_ind-275, :])))\n",
        "        Svals.append(tr_EC_d[i, j])\n",
        "\n",
        "Q = np.array(Q).astype('float32')\n",
        "X = np.array(X).astype('float32')\n",
        "Svals = np.array(Svals).astype('float32').flatten()[:, None]\n",
        "\n",
        "## Testing data\n",
        "tst_EC_d = All[tst_indices, 5:]\n",
        "tst_EC_d = np.flip(tst_EC_d, 1)\n",
        "tst_EC_d = (tst_EC_d - EC_min) / (EC_max - EC_min) # Normalize\n",
        "\n",
        "Q_tst = []  # Validation inputs for ANN\n",
        "X_tst = []  # Validation inputs for PINN\n",
        "Svals_tst = []  # Validation outputs\n",
        "\n",
        "for i in range(len(tst_EC_d)):\n",
        "    cur_ind = tst_indices[i]\n",
        "    for j in range(3):\n",
        "        # Martinez\n",
        "        if j == 0:\n",
        "            Q_tst.append(M_OUT[cur_ind-275, :])\n",
        "            X_tst.append(np.concatenate(([0, (cur_ind - 274) * portion], M_OUT[cur_ind-275, :])))\n",
        "        # Port Chicago\n",
        "        elif j == 1:\n",
        "            Q_tst.append(P_OUT[cur_ind-275, :])\n",
        "            X_tst.append(np.concatenate(([0.442, (cur_ind - 274) * portion], P_OUT[cur_ind-275, :])))\n",
        "        # Chipps Island\n",
        "        elif j == 2:\n",
        "            Q_tst.append(C_OUT[cur_ind-275, :])\n",
        "            X_tst.append(np.concatenate(([1, (cur_ind - 274) * portion], C_OUT[cur_ind-275, :])))\n",
        "        Svals_tst.append(tst_EC_d[i, j])\n",
        "\n",
        "Q_tst = np.array(Q_tst).astype('float32')\n",
        "X_tst = np.array(X_tst).astype('float32')\n",
        "Svals_tst = np.array(Svals_tst).astype('float32').flatten()[:, None]"
      ],
      "metadata": {
        "id": "W1NrLBuiCwlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Tuner"
      ],
      "metadata": {
        "id": "gly8eFEvDICL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN model"
      ],
      "metadata": {
        "id": "D66JUTDznXYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set hyperparameters search space"
      ],
      "metadata": {
        "id": "Xt2Bzifd1Ixf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Neural Network structure:\n",
        "    1. Number of layers is preset to be\n",
        "\n",
        "                    Act 1                Act 2                Act 3\n",
        "        Input Layer ----> Hidden Layer 1 ----> Hidden Layer 2 ----> Output Layer\n",
        "    \n",
        "    2. Possible number of neurons for Hidden Layer 1 is [4, 8, 12, ..., 32]\n",
        "    3. Possible number of neurons for Hidden Layer 2 is [2, 4, 6, ..., 16]\n",
        "    4. Possible activation function for all three activation functions is [\"relu\", \"tanh\", \"elu\", \"sigmoid\"]\n",
        "'''\n",
        "\n",
        "def build_model(hp):\n",
        "    # Hyperparameters\n",
        "    act_func_1=hp.Choice(\"activation_1\", [\"relu\",\"tanh\",\"elu\",\"sigmoid\"])\n",
        "    act_func_2=hp.Choice(\"activation_2\", [\"relu\",\"tanh\",\"elu\",\"sigmoid\"])\n",
        "    act_func_3=hp.Choice(\"activation_3\", [\"relu\",\"tanh\",\"elu\",\"sigmoid\"])\n",
        "    hidden_units_1 = hp.Int(\"nhidden1\", min_value=4, max_value=32, step=4)\n",
        "    hidden_units_2 = hp.Int(\"nhidden2\", min_value=2, max_value=16, step=2)\n",
        "        \n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.Input(shape=(18,)),\n",
        "            keras.layers.Dense(hidden_units_1, activation=act_func_1),\n",
        "            keras.layers.Dense(hidden_units_2, activation=act_func_2),\n",
        "            keras.layers.Dense(1, activation=act_func_3)\n",
        "        ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(\n",
        "        learning_rate=0.001), loss=\"mse\")\n",
        "    return model\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnOvtBgqDJs1",
        "outputId": "0e886561-5d99-4ba5-d5ba-9f703d19a7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7fadf19714f0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_loss\", # Find a set of hyperparameters that minimizes the validation loss\n",
        "    max_trials=50,  # Randomly search for 50 sets\n",
        "    executions_per_trial=2, # For each set, run twice\n",
        "    # overwrite=True,\n",
        "    # directory='tuner',\n",
        "    # project_name=\"ann_dsm2\",\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mpcvlEPRnTA",
        "outputId": "91faf4d9-4fed-44bd-b5b8-04596ba5d491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 5\n",
            "activation_1 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'elu', 'sigmoid'], 'ordered': False}\n",
            "activation_2 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'elu', 'sigmoid'], 'ordered': False}\n",
            "activation_3 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'elu', 'sigmoid'], 'ordered': False}\n",
            "nhidden1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 32, 'step': 4, 'sampling': None}\n",
            "nhidden2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 16, 'step': 2, 'sampling': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search"
      ],
      "metadata": {
        "id": "S-JnTsbT5BRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(Q, Svals,\n",
        "                epochs=3000,\n",
        "                batch_size=len(Q),  # entire training for 1 epoch\n",
        "                validation_data=(Q_tst, Svals_tst),\n",
        "                callbacks=[\n",
        "                    keras.callbacks.EarlyStopping(\n",
        "                        monitor=\"val_loss\", patience=50, mode=\"min\", restore_best_weights=True),\n",
        "                    #tensorboard_cb\n",
        "                ],\n",
        "                 verbose=0\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T809Qzm3nicK",
        "outputId": "e516541e-c0da-4e42-8875-2a04186ba0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 10 best sets of hyperparameters\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE1-jInnoq_j",
        "outputId": "2f910292-ae57-4f5d-934c-d1a3df8e22a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7fadf1d353a0>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: tanh\n",
            "activation_3: elu\n",
            "nhidden1: 24\n",
            "nhidden2: 10\n",
            "Score: 0.0077597759664058685\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: relu\n",
            "activation_3: elu\n",
            "nhidden1: 8\n",
            "nhidden2: 14\n",
            "Score: 0.0077623603865504265\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: tanh\n",
            "activation_3: elu\n",
            "nhidden1: 20\n",
            "nhidden2: 12\n",
            "Score: 0.007827656110748649\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: elu\n",
            "activation_3: tanh\n",
            "nhidden1: 24\n",
            "nhidden2: 14\n",
            "Score: 0.007977251894772053\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: elu\n",
            "activation_2: relu\n",
            "activation_3: elu\n",
            "nhidden1: 12\n",
            "nhidden2: 12\n",
            "Score: 0.008038962725549936\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: elu\n",
            "activation_2: relu\n",
            "activation_3: tanh\n",
            "nhidden1: 24\n",
            "nhidden2: 14\n",
            "Score: 0.00811866857111454\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: elu\n",
            "activation_3: tanh\n",
            "nhidden1: 28\n",
            "nhidden2: 14\n",
            "Score: 0.008161781821399927\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: elu\n",
            "activation_2: tanh\n",
            "activation_3: elu\n",
            "nhidden1: 28\n",
            "nhidden2: 8\n",
            "Score: 0.008270848076790571\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: tanh\n",
            "activation_3: relu\n",
            "nhidden1: 8\n",
            "nhidden2: 10\n",
            "Score: 0.008400087710469961\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: tanh\n",
            "activation_2: relu\n",
            "activation_3: sigmoid\n",
            "nhidden1: 28\n",
            "nhidden2: 8\n",
            "Score: 0.008648994378745556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PINN model"
      ],
      "metadata": {
        "id": "AeSgT4JXndE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set hyperparameters search space"
      ],
      "metadata": {
        "id": "ntw8yYS5Hk8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Custom loss function for PINN\n",
        "    Mean squared error + PDE (1D Advection Dispersion) loss\n",
        "\"\"\"\n",
        "def PINN_loss(inp):\n",
        "    def loss(y_true, y_pred):\n",
        "        A = 0.002\n",
        "        K = 133.959\n",
        "        S = y_pred\n",
        "        Q = inp[2:3]\n",
        "        dS_x = dde.grad.jacobian(S, inp, i=0, j=0)\n",
        "        dS_t = dde.grad.jacobian(S, inp, i=0, j=1)\n",
        "        dS_xx = dde.grad.hessian(S, inp, i=0, j=0)\n",
        "        mse = tf.keras.losses.MeanSquaredError()\n",
        "        return mse(y_true, S) + ((A * dS_t - Q * dS_x - K * A * dS_xx) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Sr4E1LmqLdId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Neural Network structure:\n",
        "    1. Number of layers is preset to be\n",
        "\n",
        "                    Act 1                Act 2                Act 3\n",
        "        Input Layer ----> Hidden Layer 1 ----> Hidden Layer 2 ----> Output Layer\n",
        "    \n",
        "    2. Possible number of neurons for Hidden Layer 1 is [4, 8, 12, ..., 32]\n",
        "    3. Possible number of neurons for Hidden Layer 2 is [2, 4, 6, ..., 16]\n",
        "    4. Possible activation function for all three activation functions is [\"relu\", \"tanh\", \"elu\", \"sigmoid\"]\n",
        "'''\n",
        "\n",
        "def build_model(hp):\n",
        "    # Hyperparameters\n",
        "    act_func_1=hp.Choice(\"activation_1\", [\"relu\",\"tanh\",\"elu\",\"sigmoid\"])\n",
        "    act_func_2=hp.Choice(\"activation_2\", [\"relu\",\"tanh\",\"elu\",\"sigmoid\"])\n",
        "    act_func_3=hp.Choice(\"activation_3\", [\"relu\",\"tanh\",\"elu\",\"sigmoid\"])\n",
        "    act_func_4=hp.Choice(\"activation_4\", [\"relu\",\"tanh\",\"elu\",\"sigmoid\"])\n",
        "    hidden_units_1 = hp.Int(\"nhidden1\", min_value=2, max_value=50, step=4)\n",
        "    hidden_units_2 = hp.Int(\"nhidden2\", min_value=2, max_value=50, step=4)\n",
        "    hidden_units_3 = hp.Int(\"nhidden3\", min_value=2, max_value=50, step=4)\n",
        "\n",
        "    Inp = keras.layers.Input(shape=(20,))\n",
        "    L1 = keras.layers.Dense(hidden_units_1, activation=act_func_1)(Inp)\n",
        "    L2 = keras.layers.Dense(hidden_units_2, activation=act_func_2)(L1)\n",
        "    L3 = keras.layers.Dense(hidden_units_3, activation=act_func_2)(L2)\n",
        "    Out = keras.layers.Dense(1, activation=act_func_4)(L3)\n",
        "    PINN_model = keras.Model(Inp, Out)\n",
        "    PINN_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=PINN_loss(Inp))\n",
        "    return PINN_model\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3TlK1p3n3BA",
        "outputId": "0b6bbc62-9cdd-40f1-942e-3e6799abe0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f1a61767f70>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_loss\", # Find a set of hyperparameters that minimizes the validation loss\n",
        "    max_trials=50,  # Randomly search for 50 sets\n",
        "    executions_per_trial=2, # For each set, run twice\n",
        "    # overwrite=True,\n",
        "    # directory='tuner',\n",
        "    # project_name=\"ann_dsm2\",\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-Raj-Gvn0aj",
        "outputId": "60f6e0b3-6ad2-4343-bc43-36b8ab67d926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 7\n",
            "activation_1 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'elu', 'sigmoid'], 'ordered': False}\n",
            "activation_2 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'elu', 'sigmoid'], 'ordered': False}\n",
            "activation_3 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'elu', 'sigmoid'], 'ordered': False}\n",
            "activation_4 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'elu', 'sigmoid'], 'ordered': False}\n",
            "nhidden1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 50, 'step': 4, 'sampling': None}\n",
            "nhidden2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 50, 'step': 4, 'sampling': None}\n",
            "nhidden3 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 50, 'step': 4, 'sampling': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search"
      ],
      "metadata": {
        "id": "SBGgJjZfIj6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X, Svals,\n",
        "                epochs=3000,\n",
        "                batch_size=len(X),  # entire training for 1 epoch\n",
        "                validation_data=(X_tst, Svals_tst),\n",
        "                callbacks=[\n",
        "                    keras.callbacks.EarlyStopping(\n",
        "                        monitor=\"val_loss\", patience=50, mode=\"min\", restore_best_weights=True),\n",
        "                    #tensorboard_cb\n",
        "                ],\n",
        "                verbose=0\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctQhdghBq_mg",
        "outputId": "ff747784-42c4-4ab8-8ee9-124b13b25ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 10 best sets of hyperparameters\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieG7gxb1fOHS",
        "outputId": "00341786-e864-4521-af7e-de368a3ba065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7f1a6049c790>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: tanh\n",
            "activation_3: tanh\n",
            "activation_4: sigmoid\n",
            "nhidden1: 10\n",
            "nhidden2: 26\n",
            "nhidden3: 14\n",
            "Score: 0.005956381559371948\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: sigmoid\n",
            "activation_2: relu\n",
            "activation_3: sigmoid\n",
            "activation_4: elu\n",
            "nhidden1: 14\n",
            "nhidden2: 30\n",
            "nhidden3: 46\n",
            "Score: 0.006114940391853452\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: sigmoid\n",
            "activation_2: relu\n",
            "activation_3: sigmoid\n",
            "activation_4: elu\n",
            "nhidden1: 14\n",
            "nhidden2: 42\n",
            "nhidden3: 26\n",
            "Score: 0.00656454567797482\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: sigmoid\n",
            "activation_2: relu\n",
            "activation_3: sigmoid\n",
            "activation_4: elu\n",
            "nhidden1: 46\n",
            "nhidden2: 6\n",
            "nhidden3: 46\n",
            "Score: 0.0067662259098142385\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: relu\n",
            "activation_2: sigmoid\n",
            "activation_3: relu\n",
            "activation_4: relu\n",
            "nhidden1: 6\n",
            "nhidden2: 2\n",
            "nhidden3: 30\n",
            "Score: 0.006785990204662085\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: tanh\n",
            "activation_2: sigmoid\n",
            "activation_3: sigmoid\n",
            "activation_4: sigmoid\n",
            "nhidden1: 18\n",
            "nhidden2: 2\n",
            "nhidden3: 18\n",
            "Score: 0.006947533925995231\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: sigmoid\n",
            "activation_2: tanh\n",
            "activation_3: relu\n",
            "activation_4: tanh\n",
            "nhidden1: 10\n",
            "nhidden2: 42\n",
            "nhidden3: 10\n",
            "Score: 0.0070724450051784515\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: tanh\n",
            "activation_2: tanh\n",
            "activation_3: tanh\n",
            "activation_4: sigmoid\n",
            "nhidden1: 10\n",
            "nhidden2: 46\n",
            "nhidden3: 22\n",
            "Score: 0.007130195386707783\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: sigmoid\n",
            "activation_2: elu\n",
            "activation_3: elu\n",
            "activation_4: tanh\n",
            "nhidden1: 22\n",
            "nhidden2: 46\n",
            "nhidden3: 22\n",
            "Score: 0.007147755939513445\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "activation_1: sigmoid\n",
            "activation_2: relu\n",
            "activation_3: sigmoid\n",
            "activation_4: sigmoid\n",
            "nhidden1: 10\n",
            "nhidden2: 46\n",
            "nhidden3: 34\n",
            "Score: 0.007226087152957916\n"
          ]
        }
      ]
    }
  ]
}